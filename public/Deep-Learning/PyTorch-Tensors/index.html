<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: January 4, 2024 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.7.0 for Hugo" />
  

  
  












  
  










  







  
  

  
  
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.d060e36f065b14306ff371728665eb02.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  

  <meta name="google-site-verification" content="Zv4l_ljWZhu4o0Z-kfZwQmuokpt40AvKXA78N8kynpc" />





<script async src="https://www.googletagmanager.com/gtag/js?id=G-55GQYC5GYC"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'G-55GQYC5GYC', {});
  gtag('set', {'cookie_flags': 'SameSite=None;Secure'});

  
  document.addEventListener('click', onClickCallback, false);
</script>




<script>
  (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','G-55GQYC5GYC');
</script>




















  
  
  






  <meta name="author" content="Arman Asgharpoor Golroudbari" />





  

<meta name="description" content="A journey into PyTorch tensors: creation, operations, gradient computation, and advanced functionalities for deep learning." />



<link rel="alternate" hreflang="en-us" href="https://armanasq.github.io/Deep-Learning/PyTorch-Tensors/" />
<link rel="canonical" href="https://armanasq.github.io/Deep-Learning/PyTorch-Tensors/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png" />
<meta property="og:site_name" content="" />
<meta property="og:url" content="https://armanasq.github.io/Deep-Learning/PyTorch-Tensors/" />
<meta property="og:title" content="A Profound Journey into PyTorch Tensors: A Comprehensive Tutorial | " />
<meta property="og:description" content="A journey into PyTorch tensors: creation, operations, gradient computation, and advanced functionalities for deep learning." /><meta property="og:image" content="https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta
      property="article:published_time"
      content="2023-06-01T00:00:00&#43;00:00"
    />
  
  
    <meta property="article:modified_time" content="2023-06-01T00:00:00&#43;00:00">
  






    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://armanasq.github.io/Deep-Learning/PyTorch-Tensors/"
  },
  "headline": "A Profound Journey into PyTorch Tensors: A Comprehensive Tutorial",
  
  "datePublished": "2023-06-01T00:00:00Z",
  "dateModified": "2023-06-01T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Arman Asgharpoor Golroudbari"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "",
    "logo": {
      "@type": "ImageObject",
      "url": "https://armanasq.github.io/media/icon_hu3a1b1aacf1bb12033635935a8f8a9863_117561_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "A journey into PyTorch tensors: creation, operations, gradient computation, and advanced functionalities for deep learning."
}
</script>

  

  




  
  
  

  
  

  


  
  <title>A Profound Journey into PyTorch Tensors: A Comprehensive Tutorial | </title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="cfd9458e0a27ecadd1e7f34caa594949" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.ec9d49ca50e4b80bdb08f0417a28ed84.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/"></a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/"></a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/robotic"><span>Robotic</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#posts"><span>Posts</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#projects"><span>Projects</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#talks"><span>Talks</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/publication"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/certificates"><span>Certificates</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="mailto:a.asgharpoor1993@gmail.com" data-toggle="tooltip" data-placement="bottom" title="Drop me an email."  aria-label="Drop me an email.">
                <i class="fas fa-envelope" aria-hidden="true"></i>
              </a>
            </li>
          
            
            <li class="nav-item d-none d-lg-inline-flex">
              <a class="nav-link" href="https://github.com/armanasq" data-toggle="tooltip" data-placement="bottom" title="Follow Me on GitHub." target="_blank" rel="noopener" aria-label="Follow Me on GitHub.">
                <i class="fab fa-github" aria-hidden="true"></i>
              </a>
            </li>
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <article class="article">

  













  

  
  
  
<div class="article-container pt-3">
  <h1>A Profound Journey into PyTorch Tensors: A Comprehensive Tutorial</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Jun 1, 2023
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    39 min read
  </span>
  

  
  
  
  

  
  

</div>

    




<div class="btn-links mb-3">
  
  








  






  
  
    
  
<a class="btn btn-outline-primary btn-page-header" href="https://colab.research.google.com/drive/1B0bRq3XpbDGOuVRi8q3ycNiR_gatfBH8?usp=sharing" target="_blank" rel="noopener">
  Code
</a>













  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/Armanasq/Deep-Learning-Tutorial/blob/main/PyTorch/Deep_Neural_Network_Implementation_Using_PyTorch.ipynb" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Project Code</a>


</div>


  
</div>



  <div class="article-container">

    <div class="article-style">
      <div style='background-color: rgba(225,225,225,0.48); padding: 10px; border-radius:15px;'>
<p>















<figure  >
  <div class="d-flex justify-content-center">
    <div class="w-100" ><img src="/PyTorch/pytorch.png" alt="png" loading="lazy" data-zoomable /></div>
  </div></figure>
</p>
</div>
<h1 id="a-profound-journey-into-pytorch-tensors-a-tutorial">A Profound Journey into PyTorch Tensors: A Tutorial</h1>
<ul>
<li><a href="#a-profound-journey-into-pytorch-tensors-a-tutorial">A Profound Journey into PyTorch Tensors: A Tutorial</a>
<ul>
<li><a href="#1-introduction-to-tensors">1. Introduction to Tensors</a>
<ul>
<li><a href="#11-what-are-tensors">1.1. What are Tensors?</a></li>
<li><a href="#12-why-use-tensors-in-pytorch">1.2. Why Use Tensors in PyTorch?</a>
<ul>
<li><a href="#121-gpu-acceleration">1.2.1. GPU Acceleration</a></li>
<li><a href="#122-automatic-differentiation">1.2.2. Automatic Differentiation</a></li>
<li><a href="#123-compatibility-with-neural-network-libraries">1.2.3. Compatibility with Neural Network Libraries</a></li>
</ul>
</li>
<li><a href="#13-tensor-notation-and-nomenclature">1.3. Tensor Notation and Nomenclature</a></li>
</ul>
</li>
<li><a href="#2-creating-tensors">2. Creating Tensors</a>
<ul>
<li><a href="#21-initialization-from-lists-and-arrays">2.1. Initialization from Lists and Arrays</a></li>
<li><a href="#22-creating-tensors-with-default-values">2.2. Creating Tensors with Default Values</a></li>
<li><a href="#23-creating-tensors-from-existing-tensors">2.3. Creating Tensors from Existing Tensors</a></li>
<li><a href="#24-data-types-and-precision">2.4. Data Types and Precision</a></li>
<li><a href="#25-tensor-attributes-and-metadata">2.5. Tensor Attributes and Metadata</a></li>
<li><a href="#26-tensor-serialization-and-io">2.6. Tensor Serialization and I/O</a></li>
</ul>
</li>
<li><a href="#3-tensor-operations-indexing-and-slicing">3. Tensor Operations: Indexing and Slicing</a>
<ul>
<li><a href="#31-basic-indexing-and-indexing-tricks">3.1. Basic Indexing and Indexing Tricks</a></li>
<li><a href="#32-advanced-indexing-integer-and-boolean">3.2. Advanced Indexing (Integer and Boolean)</a></li>
<li><a href="#31-basic-indexing-and-indexing-tricks-1">3.1. Basic Indexing and Indexing Tricks</a>
<ul>
<li><a href="#311-basic-indexing">3.1.1. Basic Indexing</a></li>
<li><a href="#312-advanced-indexing-with-integer-arrays">3.1.2. Advanced Indexing with Integer Arrays</a></li>
<li><a href="#313-boolean-array-indexing">3.1.3. Boolean Array Indexing</a></li>
</ul>
</li>
<li><a href="#33-modifying-values-using-indexing">3.3. Modifying Values Using Indexing</a></li>
<li><a href="#34-slicing-and-striding-explained">3.4. Slicing and Striding Explained</a>
<ul>
<li><a href="#341-slicing-to-extract-sub-tensors">3.4.1. Slicing to Extract Sub-tensors</a></li>
<li><a href="#342-striding-to-skip-elements-during-slicing">3.4.2. Striding to Skip Elements during Slicing</a></li>
</ul>
</li>
<li><a href="#35-in-place-vs-out-of-place-operations">3.5. In-place vs. Out-of-place Operations</a>
<ul>
<li><a href="#351-in-place-operations">3.5.1. In-place Operations</a></li>
<li><a href="#352-out-of-place-operations">3.5.2. Out-of-place Operations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#4-element-wise-tensor-operations">4. Element-wise Tensor Operations</a>
<ul>
<li><a href="#41-arithmetic-operations">4.1. Arithmetic Operations</a></li>
<li><a href="#42-element-wise-mathematical-functions">4.2. Element-wise Mathematical Functions</a></li>
<li><a href="#43-comparison-operations">4.3. Comparison Operations</a></li>
<li><a href="#44-clipping-tensors">4.4. Clipping Tensors</a></li>
<li><a href="#45-handling-nan-and-inf">4.5. Handling NaN and Inf</a></li>
</ul>
</li>
<li><a href="#5-tensor-broadcasting">5. Tensor Broadcasting</a>
<ul>
<li><a href="#51-broadcasting-rules-and-broadcasting-dimensions">5.1. Broadcasting Rules and Broadcasting Dimensions</a></li>
<li><a href="#52-broadcasting-examples-and-common-pitfalls">5.2. Broadcasting Examples and Common Pitfalls</a></li>
<li><a href="#53-broadcasting-vs-tile-and-expand">5.3. Broadcasting vs. Tile and Expand</a></li>
</ul>
</li>
<li><a href="#6-working-with-devices-cpu-and-gpu">6. Working with Devices (CPU and GPU)</a>
<ul>
<li><a href="#61-device-configuration-and-availability">6.1. Device Configuration and Availability</a></li>
<li><a href="#62-moving-tensors-between-devices">6.2. Moving Tensors Between Devices</a></li>
<li><a href="#63-using-mixed-precision-half-and-single">6.3. Using Mixed Precision (Half and Single)</a></li>
</ul>
</li>
<li><a href="#7-tensor-creation-methods">7. Tensor Creation Methods</a>
<ul>
<li><a href="#71-zeros-and-ones-tensors">7.1. Zeros and Ones Tensors</a></li>
<li><a href="#72-identity-and-diagonal-tensors">7.2. Identity and Diagonal Tensors</a></li>
<li><a href="#73-range-and-linspace-tensors">7.3. Range and Linspace Tensors</a></li>
<li><a href="#74-logspace-and-exponential-tensors">7.4. Logspace and Exponential Tensors</a></li>
<li><a href="#75-random-tensors-uniform-normal-and-more">7.5. Random Tensors (Uniform, Normal, and more)</a></li>
<li><a href="#76-loading-data-from-numpy-arrays">7.6. Loading Data from NumPy Arrays</a></li>
</ul>
</li>
<li><a href="#8-tensor-reshaping-and-dimensionality">8. Tensor Reshaping and Dimensionality</a>
<ul>
<li><a href="#81-reshaping-tensors">8.1. Reshaping Tensors</a></li>
<li><a href="#82-transposing-and-permuting-dimensions">8.2. Transposing and Permuting Dimensions</a></li>
<li><a href="#83-squeezing-and-unsqueezing">8.3. Squeezing and Unsqueezing</a></li>
<li><a href="#84-flattening-and-raveling-tensors">8.4. Flattening and Raveling Tensors</a></li>
<li><a href="#85-concatenating-and-stacking-tensors">8.5. Concatenating and Stacking Tensors</a></li>
</ul>
</li>
<li><a href="#9-tensor-reduction-operations">9. Tensor Reduction Operations</a>
<ul>
<li><a href="#91-summation-and-mean">9.1. Summation and Mean</a></li>
<li><a href="#92-minimum-and-maximum">9.2. Minimum and Maximum</a></li>
<li><a href="#93-argmin-and-argmax">9.3. Argmin and Argmax</a></li>
<li><a href="#94-reductions-along-specific-axes">9.4. Reductions Along Specific Axes</a></li>
<li><a href="#95-logical-reductions-all-any">9.5. Logical Reductions (All, Any)</a></li>
</ul>
</li>
<li><a href="#10-gradient-computation-and-autograd">10. Gradient Computation and Autograd</a>
<ul>
<li><a href="#101-automatic-differentiation-in-pytorch">10.1. Automatic Differentiation in PyTorch</a></li>
<li><a href="#102-computing-gradients-with-autograd">10.2. Computing Gradients with Autograd</a></li>
<li><a href="#103-detaching-tensors-from-autograd">10.3. Detaching Tensors from Autograd</a></li>
<li><a href="#104-working-with-require_grad-and-volatile">10.4. Working with <code>require_grad</code> and <code>volatile</code></a></li>
</ul>
</li>
<li><a href="#11-tensor-operations-in-advanced-topics">11. Tensor Operations in Advanced Topics</a>
<ul>
<li><a href="#111-advanced-broadcasting-and-einsum">11.1. Advanced Broadcasting and Einsum</a></li>
<li><a href="#112-tensor-concatenation-and-splitting">11.2. Tensor Concatenation and Splitting</a></li>
<li><a href="#113-masked-operations-and-scatter-gather">11.3. Masked Operations and Scatter-Gather</a></li>
<li><a href="#114-advanced-element-wise-operations">11.4. Advanced Element-wise Operations</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
<p>Welcome to this comprehensive and scholarly tutorial on the intriguing domain of PyTorch tensors. In this academic exposition, we will delve deep into the intricate intricacies of tensors, encompassing their creation, mathematical operations, and advanced functionalities, all within the context of deep learning. This tutorial aims to provide a rigorous and technical understanding of PyTorch tensors, enabling you to effectively harness their power for various machine learning endeavors.</p>
<p>As we embark on this scholarly journey, we shall navigate through the foundational concepts and theoretical underpinnings of tensors. Emphasis will be placed on their rigorous mathematical representations, properties, and significance in the context of PyTorch.</p>
<p>Throughout this academic exploration, we shall familiarize ourselves with the celestial realm of PyTorch, an esteemed open-source machine learning library renowned for its capabilities in neural network research. We will delve into the realm of automatic differentiation, a powerful tool that empowers us to efficiently compute gradients and optimize complex models with utmost precision.</p>
<p>Additionally, we shall appreciate the ethereal elegance of GPU acceleration, a pivotal technology that accelerates tensor operations, thereby facilitating expedited and more efficient computations for deep learning tasks. This integration of hardware acceleration elevates the performance of neural networks to unparalleled heights, enabling cutting-edge research and applications.</p>
<p>Our scholarly pursuit will extend to explore the vast landscape of neural network libraries, harmonizing with PyTorch&rsquo;s interoperability and compatibility. Understanding these integrated ecosystems will expand our repertoire, equipping us to develop sophisticated and adaptive models that are seamlessly integrated with state-of-the-art architectures.</p>
<p>Throughout this intellectual odyssey, we shall engage with the nomenclature and notations that embellish tensors, offering profound insights into their inherent structure and characteristics. The meticulous examination of scalar, vector, and matrix tensors will serve as foundational stepping stones to unravel the complexities of higher-dimensional tensors, where abstract structures unfold, enriching our understanding of the mathematical abstractions at play.</p>
<p>Each stage of this academic journey will unlock the boundless potential of PyTorch tensors, empowering us to ascend to greater heights in the realm of deep learning. Here, we shall appreciate the artistry of data manipulation, where tensors become the canvas, and equations compose symphonies of machine intelligence.</p>
<p>With each lesson imbibed, we shall acquire the knowledge and expertise to harness the true essence of PyTorch tensors. Armed with this proficiency, we shall endeavor to craft intelligent machines capable of reshaping the world through groundbreaking advancements in artificial intelligence.</p>
<p>Are you prepared to embark on this academic odyssey? Let us immerse ourselves in the scholarly world of PyTorch tensors, where discoveries await at every juncture. Let us commence this intellectual quest with unwavering zeal, a thirst for knowledge, and a dedication to unraveling the mysteries of tensors in the realm of deep learning. The journey awaits!</p>
<h2 id="1-introduction-to-tensors">1. Introduction to Tensors</h2>
<h3 id="11-what-are-tensors">1.1. What are Tensors?</h3>
<p>Tensors, in the realm of PyTorch, are profound mathematical abstractions that transcend the traditional confines of scalars, vectors, and matrices. As a powerful generalization of these fundamental entities, tensors materialize as multi-dimensional arrays, extending their embrace to higher-order data structures. Just as vectors can be imagined as 1-dimensional arrays and matrices as 2-dimensional arrays, tensors extend this concept further, encompassing arrays with any number of dimensions.</p>
<p>Formally, a tensor of rank <em>n</em> is defined as a multi-dimensional array of <em>n</em>-th order, consisting of <em>n</em> indices. Each index represents the position of an element in the tensor along a specific dimension. The dimensions, or &ldquo;axes,&rdquo; of the tensor dictate its rank. Scalars, devoid of dimensions, manifest as 0-dimensional tensors (<em>n=0</em>), while vectors, endowed with a single dimension, flourish as 1-dimensional tensors (<em>n=1</em>). In analogous fashion, matrices, luxuriating in two dimensions, thrive as 2-dimensional tensors (<em>n=2</em>). Beyond this realm of familiar entities, tensors elegantly transcend to higher dimensions, gaining an inherent flexibility to capture and process complex data structures.</p>
<p>Let&rsquo;s denote a tensor as <strong>T</strong> with its elements represented by <em>T</em><sub>i_1, i_2, &hellip;, i_n</sub>, where <em>i_1, i_2, &hellip;, i_n</em> are the indices along each axis. The shape of the tensor is represented by the tuple <em>(d_1, d_2, &hellip;, d_n)</em>, where <em>d_i</em> represents the number of elements along the <em>i</em>-th dimension.</p>
<p>A few noteworthy examples to solidify our understanding:</p>
<ol>
<li>
<p>Scalar (0-dimensional tensor): A scalar, often denoted as <em>a</em> or <em>A</em>, is a single numerical value without any dimensions. Mathematically, it can be expressed as <strong>a</strong> or <strong>A</strong>.</p>
</li>
<li>
<p>Vector (1-dimensional tensor): A vector, represented by <strong>v</strong> or <strong>V</strong>, consists of a collection of elements aligned along a single dimension. The elements are indexed from 1 to <em>n</em>, where <em>n</em> denotes the length of the vector. For example, <strong>v</strong> = [3, 1, 4, 1, 5] is a 1-dimensional tensor of length 5.</p>
</li>
<li>
<p>Matrix (2-dimensional tensor): A matrix, denoted by <strong>M</strong>, is an array with two dimensions, organized in rows and columns. Its elements are indexed as <em>M</em><sub>ij</sub>, where <em>i</em> and <em>j</em> represent the row and column indices, respectively. For instance:</p>
<p><strong>M</strong> = | 2  9  4 |
| 7  5  3 |</p>
</li>
<li>
<p>Higher-dimensional tensor (e.g., 3-dimensional tensor): A tensor can transcend beyond matrices into the realm of higher dimensions. Consider a 3-dimensional tensor <strong>T</strong> with elements <em>T</em><sub>ijk</sub>, where <em>i</em>, <em>j</em>, and <em>k</em> represent the three indices along each axis.</p>
</li>
</ol>
<p>Tensors serve as the fundamental building blocks for housing and manipulating data in PyTorch. They form the bedrock of computational graphs, the conceptual foundation underlying the process of automatic differentiationâ€”undeniably one of the cornerstones of contemporary deep learning frameworks. By natively integrating with automatic differentiation, PyTorch facilitates the automatic computation of gradients during the backward pass of the training process, enabling efficient gradient-based optimization and model learning.</p>
<h3 id="12-why-use-tensors-in-pytorch">1.2. Why Use Tensors in PyTorch?</h3>
<p>PyTorch, a celebrated open-source machine learning library, bestows upon researchers and practitioners an array of advantages through its seamless integration of tensors. By harnessing the full potential of PyTorch tensors, we unlock a multitude of features that elevate our machine learning endeavors to unprecedented heights.</p>
<h4 id="121-gpu-acceleration">1.2.1. GPU Acceleration</h4>
<p>In the pursuit of unrivaled computational performance, PyTorch harnesses the immense processing power of Graphics Processing Units (GPUs). GPUs, originally designed for rendering graphics, excel in parallel computations. The parallel architecture of GPUs lends itself exceptionally well to the matrix and tensor operations that pervade the landscape of deep learning. By harnessing the GPU&rsquo;s prowess, PyTorch unlocks a realm of swiftness and efficiency, significantly reducing training times and empowering researchers to tackle data-intensive challenges with newfound agility.</p>
<h4 id="122-automatic-differentiation">1.2.2. Automatic Differentiation</h4>
<p>Among PyTorch&rsquo;s most distinguished features is its unwavering commitment to automatic differentiation. As a neural network embarks on its forward pass, PyTorch dynamically constructs a computational graph that meticulously traces all tensor operations. This graph, ingeniously crafted in the backdrop, serves as the latticework for the backward pass during gradient computation. By deftly automating gradient computations, PyTorch empowers researchers to focus on the creative aspects of model development, unshackling them from the burden of manual derivative calculations.</p>
<h4 id="123-compatibility-with-neural-network-libraries">1.2.3. Compatibility with Neural Network Libraries</h4>
<p>PyTorch thrives in a rich ecosystem of neural network libraries, most notably torchvision and torchtext, to name a few. This vibrant ecosystem extends a trove of pre-built model architectures, datasets, and other utilities that amplify research productivity. Through seamless integration with these libraries, practitioners can accelerate experimentation, leveraging existing components to prototype innovative ideas efficiently. Additionally, PyTorch&rsquo;s thriving community ensures the dissemination of the latest research and techniques, fostering a dynamic environment of continuous learning and innovation.</p>
<h3 id="13-tensor-notation-and-nomenclature">1.3. Tensor Notation and Nomenclature</h3>
<p>Before embarking on practical implementations, it is paramount to grasp the tensor notation and naming conventions used in PyTorch. These conventions lay the groundwork for a profound comprehension of PyTorch&rsquo;s inner workings and enable seamless navigation of documentation, code examples, and collaborations within the PyTorch community.</p>
<p>Tensors adopt an intuitive and systematic notation, with their labels contingent on the number of dimensions or axes (rank) they possess:</p>
<ul>
<li>Scalars, representing 0-dimensional tensors, are denoted as <strong>T</strong> or <strong>T0</strong>.</li>
<li>Vectors, embodying 1-dimensional tensors, are denoted as <strong>T</strong> or <strong>T1</strong>.</li>
<li>Matrices, exemplifying 2-dimensional tensors, are denoted as <strong>T</strong> or <strong>T2</strong>.</li>
<li>Higher-dimensional tensors, such as 3-dimensional or 4-dimensional tensors, are aptly labeled as <strong>T3</strong> and <strong>T4</strong>, respectively.</li>
</ul>
<p>Moreover, tensors possess a shape, represented by a tuple that specifies the number of elements along each axis. For instance, a 3x2 matrix would have a shape of <em>(3, 2)</em>, denoting 3 rows and 2 columns.</p>
<p>By mastering these tensor notations and conventions, we lay a</p>
<p>robust foundation for efficient comprehension and manipulation of tensors, embarking on a compelling journey into the profound realm of PyTorch.</p>
<h2 id="2-creating-tensors">2. Creating Tensors</h2>
<p>In the realm of PyTorch, the creation of tensors is the foundational step that ignites the journey into the world of deep learning. In this section, we will embark on an exploration of various techniques for tensor creation, enabling us to wield this powerful tool effectively.</p>
<h3 id="21-initialization-from-lists-and-arrays">2.1. Initialization from Lists and Arrays</h3>
<p>In PyTorch, tensors can be effortlessly crafted from Python lists and NumPy arrays. This seamless integration with Python data structures provides a convenient pathway to convert existing data into tensors, facilitating seamless interoperability with other Python libraries. Let us delve into the process of tensor creation using lists and arrays, while also exploring the notion of data types and device options.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for tensor creation from lists and arrays</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create tensors from lists</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_from_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create tensors from NumPy arrays</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="n">numpy_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_from_numpy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Specifying data types and device options</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_float_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>As demonstrated in the code above, we can effortlessly transform Python lists and NumPy arrays into PyTorch tensors using <code>torch.tensor()</code>. The <code>tensor_from_list</code> represents a 1-dimensional tensor, while <code>tensor_from_numpy</code> showcases how NumPy arrays can be seamlessly converted into PyTorch tensors.</p>
<p>Moreover, PyTorch tensors offer the flexibility to specify the data type of the tensor using the <code>dtype</code> parameter. This feature becomes especially important when working with specific precision requirements in numerical computations. Additionally, the ability to assign tensors to specific devices, such as GPUs (Graphics Processing Units), unlocks the potential for accelerated computation on parallel hardware.</p>
<h3 id="22-creating-tensors-with-default-values">2.2. Creating Tensors with Default Values</h3>
<p>Efficiency and convenience are at the core of PyTorch tensor creation. Often, we need to initialize tensors with default values, such as zeros, ones, or random numbers, for various machine learning tasks. PyTorch provides intuitive methods to achieve this, catering to diverse use cases.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating tensors with default values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a zero tensor of size (3, 4)</span>
</span></span><span class="line"><span class="cl"><span class="n">zero_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a ones tensor of size (2, 2, 2) with dtype float</span>
</span></span><span class="line"><span class="cl"><span class="n">ones_tensor_float</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor of size (5, 5) with random values from a normal distribution</span>
</span></span><span class="line"><span class="cl"><span class="n">random_normal_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</span></span></code></pre></div><p>In the code snippet above, we utilize the functions <code>torch.zeros()</code>, <code>torch.ones()</code>, and <code>torch.randn()</code> to create tensors initialized with zeros, ones, and random values from a normal distribution, respectively. These tensor initialization techniques substantially expedite the process of setting up initial values for various machine learning models and experiments.</p>
<h3 id="23-creating-tensors-from-existing-tensors">2.3. Creating Tensors from Existing Tensors</h3>
<p>The world of PyTorch tensors embraces the concept of tensor manipulation, wherein new tensors can be generated through various operations on existing tensors. Such techniques offer immense versatility in crafting complex structures from foundational tensor building blocks.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating tensors from existing tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a new tensor by cloning an existing tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">original_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">cloned_tensor</span> <span class="o">=</span> <span class="n">original_tensor</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a new tensor by reshaping an existing tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">reshaped_tensor</span> <span class="o">=</span> <span class="n">original_tensor</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a new tensor by concatenating two existing tensors</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">concatenated_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">))</span>
</span></span></code></pre></div><p>In the provided code examples, we showcase three distinct methods of creating tensors from existing tensors:</p>
<ol>
<li>
<p><strong>Cloning</strong>: The <code>clone()</code> method allows us to create an identical copy of the original tensor. This is a useful technique to preserve the values of the original tensor while performing subsequent operations.</p>
</li>
<li>
<p><strong>Reshaping</strong>: The <code>view()</code> method facilitates tensor reshaping, enabling us to alter the dimensions of a tensor while maintaining the same number of elements. Proper understanding of tensor reshaping is crucial for transforming data into formats compatible with various neural network architectures.</p>
</li>
<li>
<p><strong>Concatenation</strong>: The <code>torch.cat()</code> function empowers us to concatenate multiple tensors along a specified dimension. This operation is essential when combining data from different sources or building larger tensors from smaller ones.</p>
</li>
</ol>
<p>The ability to create new tensors through cloning, reshaping, and concatenation forms the foundation for building complex data structures in deep learning models.</p>
<h3 id="24-data-types-and-precision">2.4. Data Types and Precision</h3>
<p>As we traverse the realm of deep learning, the choice of data types for tensors plays a pivotal role in determining the computational accuracy and efficiency of our models. PyTorch offers a rich selection of data types, catering to a diverse array of numerical precision requirements.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for working with data types and precision</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Specify data types for tensors</span>
</span></span><span class="line"><span class="cl"><span class="n">float_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">int_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Perform operations with different data types</span>
</span></span><span class="line"><span class="cl"><span class="n">result_tensor</span> <span class="o">=</span> <span class="n">float_tensor</span> <span class="o">*</span> <span class="n">int_tensor</span>
</span></span></code></pre></div><p>In the above code snippet, we showcase the utilization of different data types for tensors. The <code>torch.float32</code> data type represents single-precision floating-point numbers, offering a balance between numerical accuracy and memory efficiency. On the other hand, the <code>torch.int64</code> data type represents 64-bit integers, which are essential for handling large integers in various computational scenarios.</p>
<p>Moreover, PyTorch tensors provide support for half-precision (float16) data types, enabling memory optimization for computations that tolerate reduced numerical precision. This is particularly valuable when working with large-scale deep learning models, as it facilitates more efficient memory usage while sacrificing minimal accuracy.</p>
<p>Understanding the nuances of data types and their precision levels equips us to make informed decisions in designing models that strike a balance between computational efficiency and numerical accuracy.</p>
<h3 id="25-tensor-attributes-and-metadata">2.5. Tensor Attributes and Metadata</h3>
<p>In the captivating domain of PyTorch tensors, attributes and metadata bestow essential characteristics and insights into the structure and properties of tensors. These attributes play a fundamental role in efficiently manipulating and processing tensors within a computational graph.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for tensor attributes and metadata</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor and inspect its attributes</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">shape</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl"><span class="n">dtype</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">dtype</span>
</span></span><span class="line"><span class="cl"><span class="n">num_elements</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
</span></span></code></pre></div><p>The code above provides a glimpse into tensor attributes and metadata. We explore the following attributes:</p>
<ol>
<li>
<p><strong>Shape</strong>: The <code>shape</code> attribute reveals the dimensions of the tensor, providing a tuple that specifies the size along each axis. Understanding the shape of tensors is vital in ensuring compatibility with various operations and neural network layers.</p>
</li>
<li>
<p><strong>Data Type (dtype)</strong>: The <code>dtype</code> attribute denotes the data type of the elements contained within the tensor. This information is crucial in avoiding data</p>
</li>
</ol>
<p>type mismatches that can lead to erroneous results in computations.</p>
<ol start="3">
<li><strong>Number of Elements (numel())</strong>: The <code>numel()</code> method provides the count of elements present in the tensor. This attribute aids in efficiently navigating through tensors and assessing their sizes.</li>
</ol>
<p>Together, these attributes furnish us with a comprehensive understanding of tensor properties, streamlining the process of manipulation and analysis within deep learning frameworks.</p>
<h3 id="26-tensor-serialization-and-io">2.6. Tensor Serialization and I/O</h3>
<p>In the pursuit of crafting robust and reproducible deep learning models, it is essential to save and load tensors to and from files. PyTorch offers convenient mechanisms for tensor serialization and I/O, allowing us to preserve the state of tensors for future use or exchange data between experiments seamlessly.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for tensor serialization and I/O</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Save a tensor to a file</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_to_save</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">tensor_to_save</span><span class="p">,</span> <span class="s1">&#39;tensor_data.pth&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load a tensor from a file</span>
</span></span><span class="line"><span class="cl"><span class="n">loaded_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;tensor_data.pth&#39;</span><span class="p">)</span>
</span></span></code></pre></div><p>In the provided code snippets, we demonstrate the process of tensor serialization and I/O. The <code>torch.save()</code> function allows us to save tensors to disk in a binary format, thereby preserving their structure and data. Subsequently, the <code>torch.load()</code> function enables us to retrieve the saved tensor from the file.</p>
<p>This capability becomes invaluable when dealing with complex deep learning models, enabling us to persist model parameters, intermediate results, and essential data for reproducibility and sharing across different experimental setups.</p>
<p>The prowess of PyTorch tensors to serialize and deserialize data empowers us to build sophisticated models with confidence, knowing that we can preserve and retrieve crucial data at different stages of the model development process.</p>
<p>With these comprehensive techniques for tensor creation, data type handling, manipulation, and serialization, we have established a solid foundation to traverse further into the mystical realm of PyTorch tensors. Armed with these powerful tools, we are prepared to embark on more profound explorations into the art and science of deep learning. Let us now venture into the subsequent sections, where we shall unravel the secrets of tensor operations, advanced functionalities, and the inner workings of neural networks. The journey continues!</p>
<h2 id="3-tensor-operations-indexing-and-slicing">3. Tensor Operations: Indexing and Slicing</h2>
<h3 id="31-basic-indexing-and-indexing-tricks">3.1. Basic Indexing and Indexing Tricks</h3>
<p>Understand basic indexing to access specific elements or sub-tensors from tensors. This section will also cover useful indexing tricks.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for basic indexing and tricks</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Indexing to access specific elements</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">element</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Modifying specific elements using indexing</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mi">7</span>
</span></span></code></pre></div><h3 id="32-advanced-indexing-integer-and-boolean">3.2. Advanced Indexing (Integer and Boolean)</h3>
<p>Learn advanced indexing techniques using integer and boolean arrays to extract or modify specific elements based on certain conditions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for advanced indexing (integer and boolean)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Integer array indexing</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="c1">## 3. Tensor Operations: Indexing and Slicing</span>
</span></span></code></pre></div><p>In the vast landscape of PyTorch tensors, indexing and slicing form the bedrock of accessing and manipulating data within tensors. This section delves into the intricacies of indexing and slicing operations, providing an array of techniques to unleash the full potential of tensor data.</p>
<h3 id="31-basic-indexing-and-indexing-tricks-1">3.1. Basic Indexing and Indexing Tricks</h3>
<h4 id="311-basic-indexing">3.1.1. Basic Indexing</h4>
<p>In PyTorch, basic indexing is akin to traditional array indexing in Python, where we can retrieve individual elements or sub-tensors using integer indices. Let&rsquo;s explore some basic indexing techniques:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for basic indexing</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor for demonstration</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Accessing individual elements</span>
</span></span><span class="line"><span class="cl"><span class="n">element</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element at (1, 1):&#34;</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span>  <span class="c1"># Output: 5</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Accessing entire rows or columns</span>
</span></span><span class="line"><span class="cl"><span class="n">row</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;First row:&#34;</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>  <span class="c1"># Output: tensor([1, 2, 3])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">column</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Third column:&#34;</span><span class="p">,</span> <span class="n">column</span><span class="p">)</span>  <span class="c1"># Output: tensor([3, 6, 9])</span>
</span></span></code></pre></div><h4 id="312-advanced-indexing-with-integer-arrays">3.1.2. Advanced Indexing with Integer Arrays</h4>
<p>PyTorch supports advanced indexing using integer arrays, which allows us to access non-contiguous elements from a tensor. This powerful technique opens up new possibilities for flexible data selection.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for advanced indexing with integer arrays</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor for demonstration</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Integer array indexing</span>
</span></span><span class="line"><span class="cl"><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">selected_elements</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Selected elements:&#34;</span><span class="p">,</span> <span class="n">selected_elements</span><span class="p">)</span>  <span class="c1"># Output: tensor([4, 6])</span>
</span></span></code></pre></div><h4 id="313-boolean-array-indexing">3.1.3. Boolean Array Indexing</h4>
<p>Boolean array indexing allows us to select elements from a tensor based on certain conditions. The resulting tensor contains elements for which the corresponding boolean value is <code>True</code>.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for boolean array indexing</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor for demonstration</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Boolean array indexing</span>
</span></span><span class="line"><span class="cl"><span class="n">boolean_indices</span> <span class="o">=</span> <span class="n">tensor_example</span> <span class="o">&gt;</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="n">selected_elements_boolean</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[</span><span class="n">boolean_indices</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Selected elements (boolean indexing):&#34;</span><span class="p">,</span> <span class="n">selected_elements_boolean</span><span class="p">)</span>  <span class="c1"># Output: tensor([4, 5, 6])</span>
</span></span></code></pre></div><h3 id="33-modifying-values-using-indexing">3.3. Modifying Values Using Indexing</h3>
<p>Indexing is not only limited to data retrieval but also facilitates modifications of tensor values. We can leverage indexing techniques to change specific elements or sub-tensors within a larger tensor.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for modifying tensor values using indexing</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor for demonstration</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Modifying tensor values using integer array indexing</span>
</span></span><span class="line"><span class="cl"><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">indices</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Modified tensor (integer array indexing):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>  <span class="c1"># Output: tensor([[ 1,  2,  3],</span>
</span></span><span class="line"><span class="cl">                      <span class="c1">#         [10,  5, 30]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Modifying tensor values using boolean array indexing</span>
</span></span><span class="line"><span class="cl"><span class="n">boolean_indices</span> <span class="o">=</span> <span class="n">tensor_example</span> <span class="o">&gt;</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span><span class="p">[</span><span class="n">boolean_indices</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Modified tensor (boolean array indexing):&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>  <span class="c1"># Output: tensor([[1, 2, 3],</span>
</span></span><span class="line"><span class="cl">                       <span class="c1">#         [0, 0, 0]])</span>
</span></span></code></pre></div><h3 id="34-slicing-and-striding-explained">3.4. Slicing and Striding Explained</h3>
<h4 id="341-slicing-to-extract-sub-tensors">3.4.1. Slicing to Extract Sub-tensors</h4>
<p>Slicing allows us to extract sub-tensors from a larger tensor. It involves specifying ranges along each dimension to define the sub-tensor boundaries.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for slicing and striding</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor for demonstration</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Slicing to extract sub-tensors</span>
</span></span><span class="line"><span class="cl"><span class="n">sub_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Sub-tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sub_tensor</span><span class="p">)</span>  <span class="c1"># Output: tensor([[2, 3],</span>
</span></span><span class="line"><span class="cl">                  <span class="c1">#         [5, 6]])</span>
</span></span></code></pre></div><h4 id="342-striding-to-skip-elements-during-slicing">3.4.2. Striding to Skip Elements during Slicing</h4>
<p>Striding involves skipping elements while slicing a tensor. It allows us to access specific elements with specified intervals.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Striding to skip elements during slicing</span>
</span></span><span class="line"><span class="cl"><span class="n">strided_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="p">[::</span><span class="mi">2</span><span class="p">,</span> <span class="p">::</span><span class="mi">2</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Strided tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">strided_tensor</span><span class="p">)</span>  <span class="c1"># Output: tensor([[1, 3],</span>
</span></span><span class="line"><span class="cl">                       <span class="c1">#         [7, 9]])</span>
</span></span></code></pre></div><h3 id="35-in-place-vs-out-of-place-operations">3.5. In-place vs. Out-of-place Operations</h3>
<h4 id="351-in-place-operations">3.5.1. In-place Operations</h4>
<p>In-place operations modify the tensor directly, altering its data in memory. These operations are suffixed with an underscore, indicating their in-place nature.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for in-place and out-of-place operations</span>
</span></span><span class="line"><span class="cl"><span class="c1"># In-place operations</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tensor after in-place operation:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>  <span class="c1"># Output: tensor([3, 4, 5])</span>
</span></span></code></pre></div><h4 id="352-out-of-place-operations">3.5.2. Out-of-place Operations</h4>
<p>Out-of-place operations create a new tensor as the result of the operation, leaving the original tensor unchanged.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Out-of-place operations</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">new_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Original tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>  <span class="c1"># Output: tensor([1, 2, 3])</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;New tensor after out-of-place operation:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">new_tensor</span><span class="p">)</span>  <span class="c1"># Output: tensor([3, 4, 5])</span>
</span></span></code></pre></div><p>Understanding the distinction between in-place and out-of-place operations is crucial, as in-place operations may lead to unintended side effects in the computation graph. It is essential to use the appropriate operation depending on the desired behavior and the preservation of tensor data.</p>
<h2 id="4-element-wise-tensor-operations">4. Element-wise Tensor Operations</h2>
<p>In the realm of PyTorch tensors, element-wise operations hold profound significance as they empower us to perform computations on individual elements across tensors. This section is dedicated to unraveling the mesmerizing world of element-wise tensor operations, where mathematical symphonies and numerical ballets intertwine.</p>
<h3 id="41-arithmetic-operations">4.1. Arithmetic Operations</h3>
<p>Element-wise arithmetic operations unleash the ability to perform basic mathematical operations on tensors, transforming data through addition, subtraction, multiplication, and division.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for element-wise arithmetic operations</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise addition</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_addition</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">+</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise addition:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_addition</span><span class="p">)</span>  <span class="c1"># Output: tensor([5, 7, 9])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise subtraction</span>
</span></span><span class="line"><span class="cl"><span class="n">result_subtraction</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">-</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise subtraction:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_subtraction</span><span class="p">)</span>  <span class="c1"># Output: tensor([-3, -3, -3])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise multiplication</span>
</span></span><span class="line"><span class="cl"><span class="n">result_multiplication</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">*</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise multiplication:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_multiplication</span><span class="p">)</span>  <span class="c1"># Output: tensor([ 4, 10, 18])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise division</span>
</span></span><span class="line"><span class="cl"><span class="n">result_division</span> <span class="o">=</span> <span class="n">tensor_b</span> <span class="o">/</span> <span class="n">tensor_a</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise division:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_division</span><span class="p">)</span>  <span class="c1"># Output: tensor([4., 2.5, 2.])</span>
</span></span></code></pre></div><h3 id="42-element-wise-mathematical-functions">4.2. Element-wise Mathematical Functions</h3>
<p>Delve into the world of element-wise mathematical functions, where tensors transform under the influence of powerful mathematical expressions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for element-wise mathematical functions</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise square root</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">16</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_sqrt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise square root:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_sqrt</span><span class="p">)</span>  <span class="c1"># Output: tensor([2., 3., 4.])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise exponential</span>
</span></span><span class="line"><span class="cl"><span class="n">result_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise exponential:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_exp</span><span class="p">)</span>  <span class="c1"># Output: tensor([5.4598e+01, 8.1031e+03, 8.8861e+06])</span>
</span></span></code></pre></div><h3 id="43-comparison-operations">4.3. Comparison Operations</h3>
<p>Element-wise comparison operations offer us the means to explore the relationships between elements within tensors, paving the way for logical assessments.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for element-wise comparison operations</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise greater than</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_gt</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">&gt;</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise greater than:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_gt</span><span class="p">)</span>  <span class="c1"># Output: tensor([False, False,  True])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise less than</span>
</span></span><span class="line"><span class="cl"><span class="n">result_lt</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">&lt;</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise less than:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_lt</span><span class="p">)</span>  <span class="c1"># Output: tensor([ True, False, False])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise equal to</span>
</span></span><span class="line"><span class="cl"><span class="n">result_eq</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">==</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Element-wise equal to:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_eq</span><span class="p">)</span>  <span class="c1"># Output: tensor([False,  True, False])</span>
</span></span></code></pre></div><h3 id="44-clipping-tensors">4.4. Clipping Tensors</h3>
<p>Clipping tensors enables us to restrict the range of tensor values within a specific boundary, allowing us to control data within desired bounds.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for clipping tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Clipping tensor values</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">clipped_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Clipped tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">clipped_tensor</span><span class="p">)</span>  <span class="c1"># Output: tensor([2, 2, 3, 4, 4])</span>
</span></span></code></pre></div><h3 id="45-handling-nan-and-inf">4.5. Handling NaN and Inf</h3>
<p>In the domain of numerical computations, the presence of NaN (Not a Number) and Inf (Infinity) values demands careful handling. This section illuminates the methods to identify and manage these exceptional values in tensors.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for handling NaN and Inf</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Handling NaN</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;NaN&#39;</span><span class="p">),</span> <span class="mf">3.0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_nan_check</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;NaN check:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_nan_check</span><span class="p">)</span>  <span class="c1"># Output: tensor([False,  True, False])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Handling Inf</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example_inf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;Inf&#39;</span><span class="p">),</span> <span class="mf">3.0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_inf_check</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">tensor_example_inf</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Inf check:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_inf_check</span><span class="p">)</span>  <span class="c1"># Output: tensor([False,  True, False])</span>
</span></span></code></pre></div><p>Navigating the realm of element-wise tensor operations empowers us to sculpt and manipulate data with artistic precision, uncovering the beauty within the numbers. Embrace the symphony of tensors and the dance of mathematics, for they shall guide us on this enthralling journey of deep learning.</p>
<h2 id="5-tensor-broadcasting">5. Tensor Broadcasting</h2>
<p>In the wondrous realm of PyTorch, tensor broadcasting reigns supreme, bestowing upon us the power to perform element-wise operations on tensors with different shapes. This section is devoted to unraveling the art of tensor broadcasting, where scalar operands metamorphose into multidimensional dancers, gracefully harmonizing with their tensor counterparts.</p>
<h3 id="51-broadcasting-rules-and-broadcasting-dimensions">5.1. Broadcasting Rules and Broadcasting Dimensions</h3>
<p>Tensor broadcasting, akin to a grand dance performance, adheres to a set of rules to gracefully accommodate tensors with varying shapes. Let us explore the intricacies of broadcasting rules and the dimensions that define this celestial choreography.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for broadcasting rules and dimensions</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Broadcasting with scalars</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">scalar_b</span> <span class="o">=</span> <span class="mi">5</span>
</span></span><span class="line"><span class="cl"><span class="n">result_broadcast_scalar</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">+</span> <span class="n">scalar_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Broadcasting with scalars:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_broadcast_scalar</span><span class="p">)</span>  <span class="c1"># Output: tensor([6, 7, 8])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Broadcasting with different shapes</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_broadcast_shape</span> <span class="o">=</span> <span class="n">tensor_c</span> <span class="o">+</span> <span class="n">tensor_d</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Broadcasting with different shapes:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_broadcast_shape</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([[11, 22, 33],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                 [14, 25, 36]])</span>
</span></span></code></pre></div><h3 id="52-broadcasting-examples-and-common-pitfalls">5.2. Broadcasting Examples and Common Pitfalls</h3>
<p>As we venture further into the enchanting world of broadcasting, we shall encounter more intricate examples that weave tensors of various dimensions into the graceful tapestry of computation. Be wary of common pitfalls that may hinder this dance of tensors, and learn the art of avoidance.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for broadcasting examples and pitfalls</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Broadcasting with multidimensional tensors</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_broadcast</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">+</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Broadcasting with multidimensional tensors:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_broadcast</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([[11, 22, 33],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                 [14, 25, 36]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Common broadcasting pitfalls</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># The following line will raise a RuntimeError</span>
</span></span><span class="line"><span class="cl"><span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">result_pitfall</span> <span class="o">=</span> <span class="n">tensor_c</span> <span class="o">+</span> <span class="n">tensor_d</span>
</span></span><span class="line"><span class="cl"><span class="k">except</span> <span class="ne">RuntimeError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s2">&#34;RuntimeError:&#34;</span><span class="p">,</span> <span class="n">e</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="53-broadcasting-vs-tile-and-expand">5.3. Broadcasting vs. Tile and Expand</h3>
<p>While broadcasting orchestrates an elegant dance of tensors, it is essential to distinguish its graceful moves from those of the tile and expand operations. Let us unravel the distinct nuances of these maneuvers.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for broadcasting vs. tile and expand</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Broadcasting example</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_broadcast</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">+</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Broadcasting example:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_broadcast</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([[11, 22, 33],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                 [14, 25, 36]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Tile and expand example</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tiled_tensor</span> <span class="o">=</span> <span class="n">tensor_c</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">expanded_tensor</span> <span class="o">=</span> <span class="n">tensor_c</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tile and expand example - Tiled Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tiled_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                 [1, 2, 3, 1, 2, 3, 1, 2, 3]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tile and expand example - Expanded Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">expanded_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([[1, 2, 3],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#                 [1, 2, 3]])</span>
</span></span></code></pre></div><p>As we gracefully waltz through the domain of tensor broadcasting, let us embrace the beauty of dimensionality and revel in the elegance of element-wise operations on tensors of varying shapes. Enthralling are the wonders of broadcasting, where scalars metamorphose into multidimensional virtuosos, and tensors harmoniously blend in a dance of numerical poetry. Remember the rules, beware the pitfalls, and cherish the distinctions between broadcasting, tiling, and expanding, for they are the steps to master the art of tensor choreography.</p>
<h2 id="6-working-with-devices-cpu-and-gpu">6. Working with Devices (CPU and GPU)</h2>
<p>In the realm of PyTorch, where the pursuit of computational excellence knows no bounds, we shall embark on a journey to harness the power of diverse devices. This section unveils the secrets of configuring devices and gracefully transitioning tensors between the celestial realms of CPU and GPU. The art of mixed precision shall be explored, where the harmonious combination of float16 and float32 unleashes the full potential of deep learning models. Moreover, we shall venture into the mystical realm of distributed data parallelism, where the collaborative efforts of multiple GPUs pave the way to accelerated training.</p>
<h3 id="61-device-configuration-and-availability">6.1. Device Configuration and Availability</h3>
<p>Before we set forth on our device-driven quest, let us ascertain the availability of the GPU, a celestial entity that often bestows us with enhanced computational prowess. In the land of PyTorch, the availability of the GPU is readily discernible.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for device configuration and availability</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check if GPU is available</span>
</span></span><span class="line"><span class="cl"><span class="n">is_gpu_available</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Is GPU available?&#34;</span><span class="p">,</span> <span class="n">is_gpu_available</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Specify device for tensor operations</span>
</span></span><span class="line"><span class="cl"><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">is_gpu_available</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_gpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tensor on GPU:&#34;</span><span class="p">,</span> <span class="n">tensor_gpu</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([1, 2, 3], device=&#39;cuda:0&#39;)</span>
</span></span></code></pre></div><h3 id="62-moving-tensors-between-devices">6.2. Moving Tensors Between Devices</h3>
<p>With devices at our disposal, we shall learn the graceful art of transporting tensors between CPU and GPU. This seamless transition shall enable us to utilize the strengths of both realms in a harmonious symphony of computation.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for moving tensors between devices</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Move tensor from CPU to GPU</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_cpu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_gpu</span> <span class="o">=</span> <span class="n">tensor_cpu</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tensor on GPU:&#34;</span><span class="p">,</span> <span class="n">tensor_gpu</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([1, 2, 3], device=&#39;cuda:0&#39;)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Move tensor from GPU to CPU</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_cpu_again</span> <span class="o">=</span> <span class="n">tensor_gpu</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tensor on CPU:&#34;</span><span class="p">,</span> <span class="n">tensor_cpu_again</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([1, 2, 3])</span>
</span></span></code></pre></div><h3 id="63-using-mixed-precision-half-and-single">6.3. Using Mixed Precision (Half and Single)</h3>
<p>As we traverse the boundaries of precision, we shall explore the realms of half (float16) and single (float32) precision, each possessing its own strengths. With mixed precision, we can leverage the best of both worlds to optimize deep learning models.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for using mixed precision</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use half (float16) precision</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_half</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Half Precision Tensor:&#34;</span><span class="p">,</span> <span class="n">tensor_half</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([1., 2., 3.], dtype=torch.float16)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use single (float32) precision</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_single</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Single Precision Tensor:&#34;</span><span class="p">,</span> <span class="n">tensor_single</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output: tensor([1., 2., 3.])</span>
</span></span></code></pre></div><h2 id="7-tensor-creation-methods">7. Tensor Creation Methods</h2>
<p>The journey through the enchanting world of PyTorch tensors continues, and in this chapter, we shall delve into the diverse methods of tensor creation. Unleash the power of zeros and ones, explore the elegance of identity and diagonal tensors, and traverse the realm of range and linspace tensors, where evenly spaced values beckon us forth.</p>
<h3 id="71-zeros-and-ones-tensors">7.1. Zeros and Ones Tensors</h3>
<p>Let us begin our adventure by creating tensors filled with the enchanting essence of zeros and ones. The incantation of zeros shall bring forth tensors of specific dimensions, while the allure of ones shall manifest tensors with grace.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating tensors initialized with zeros and ones</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor of zeros with size (2, 3)</span>
</span></span><span class="line"><span class="cl"><span class="n">zeros_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Zeros Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">zeros_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[0., 0., 0.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [0., 0., 0.]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor of ones with size (3, 2, 2) and float32 data type</span>
</span></span><span class="line"><span class="cl"><span class="n">ones_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Ones Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">ones_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[1., 1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [1., 1.]],</span>
</span></span><span class="line"><span class="cl"><span class="c1"># </span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [[1., 1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [1., 1.]],</span>
</span></span><span class="line"><span class="cl"><span class="c1"># </span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [[1., 1.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#          [1., 1.]]])</span>
</span></span></code></pre></div><h3 id="72-identity-and-diagonal-tensors">7.2. Identity and Diagonal Tensors</h3>
<p>In our pursuit of tensor sorcery, we shall uncover the secrets of creating identity and diagonal tensors. The mystical identity matrix shall emerge, as well as tensors with diagonal values imbued with meaning.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating identity and diagonal tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create an identity matrix of size (3, 3)</span>
</span></span><span class="line"><span class="cl"><span class="n">identity_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Identity Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">identity_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1., 0., 0.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [0., 1., 0.],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [0., 0., 1.]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a diagonal tensor with diagonal values (1, 2, 3)</span>
</span></span><span class="line"><span class="cl"><span class="n">diagonal_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">diagonal_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">diagonal_values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Diagonal Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">diagonal_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 0, 0],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [0, 2, 0],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [0, 0, 3]])</span>
</span></span></code></pre></div><h3 id="73-range-and-linspace-tensors">7.3. Range and Linspace Tensors</h3>
<p>Behold the power of range and linspace tensors, where evenly spaced values gracefully present themselves. The art of range tensors crafts values with a given step, while the allure of linspace tensors harmonizes a specified number of linearly spaced values within a range.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating tensors with evenly spaced values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with values from 0 to 4 (exclusive) with a step of 1</span>
</span></span><span class="line"><span class="cl"><span class="n">range_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Range Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">range_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([0, 1, 2, 3])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with 5 values linearly spaced between 0 and 1 (inclusive)</span>
</span></span><span class="line"><span class="cl"><span class="n">linspace_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Linspace Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">linspace_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])</span>
</span></span></code></pre></div><h3 id="74-logspace-and-exponential-tensors">7.4. Logspace and Exponential Tensors</h3>
<p>Venture further into the depths of tensor magic with logspace and exponential tensors, where logarithmically and exponentially spaced values emerge. The logspace tensor crafts values with a logarithmic progression, while the exponential tensor conjures values with an exponential allure.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating tensors with logarithmically and exponentially spaced values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with 5 logarithmically spaced values between 1e-2 and 1e2 (inclusive)</span>
</span></span><span class="line"><span class="cl"><span class="n">logspace_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Logarithmically Spaced Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">logspace_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1.0000e-02, 1.0000e-01, 1.0000e+00, 1.0000e+01, 1.0000e+02])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with 4 exponentially spaced values between 2 and 32 (inclusive)</span>
</span></span><span class="line"><span class="cl"><span class="n">start_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">10.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">end_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">100.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">exp_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">start_value</span><span class="p">),</span> 
</span></span><span class="line"><span class="cl">                            <span class="n">end</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">end_value</span><span class="p">),</span> <span class="n">steps</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Exponential Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">exp_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 10.0000,  17.7828,  31.6228,  56.2341, 100.0000])</span>
</span></span></code></pre></div><h3 id="75-random-tensors-uniform-normal-and-more">7.5. Random Tensors (Uniform, Normal, and more)</h3>
<p>Now, let us immerse ourselves in</p>
<p>the mystical world of random tensors, where values are summoned from various enchanting distributions. Behold the creation of tensors with random values from the uniform and normal realms, and witness the allure of the discrete uniform distribution.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for creating tensors with random values</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with random values from a uniform distribution between 0 and 1</span>
</span></span><span class="line"><span class="cl"><span class="n">uniform_random_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Uniform Random Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">uniform_random_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[0.2209, 0.7670, 0.6110],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [0.6391, 0.7407, 0.2386]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with random values from a normal distribution with mean 0 and standard deviation 1</span>
</span></span><span class="line"><span class="cl"><span class="n">normal_random_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Normal Random Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">normal_random_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[-0.1452, -0.5339, -1.0063],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1.1233, -0.3231, -1.2437],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [-0.4005,  0.0379, -0.4895]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a tensor with random values from a discrete uniform distribution between 1 and 10</span>
</span></span><span class="line"><span class="cl"><span class="n">discrete_uniform_random_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Discrete Uniform Random Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">discrete_uniform_random_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[ 2,  9],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 1, 10]])</span>
</span></span></code></pre></div><h3 id="76-loading-data-from-numpy-arrays">7.6. Loading Data from NumPy Arrays</h3>
<p>In our quest for knowledge, let us bridge the realms of NumPy and PyTorch, for their harmonious cooperation shall grant us greater insight. Embrace the art of loading data from NumPy arrays into PyTorch tensors, as we unify the power of two enchanting worlds.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for loading data from NumPy arrays</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Create a NumPy array</span>
</span></span><span class="line"><span class="cl"><span class="n">numpy_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Load the NumPy array into a PyTorch tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_from_numpy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Tensor from NumPy Array:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">tensor_from_numpy</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 2, 3, 4, 5])</span>
</span></span></code></pre></div><h2 id="8-tensor-reshaping-and-dimensionality">8. Tensor Reshaping and Dimensionality</h2>
<p>Our journey now ventures into the art of tensor reshaping and dimensionality, where the fabric of tensors takes on new forms. Witness the elegant transformation of tensors through reshaping, transposing, squeezing, and unsqueezing, as we unravel their multidimensional nature.</p>
<h3 id="81-reshaping-tensors">8.1. Reshaping Tensors</h3>
<p>Behold the magical art of reshaping tensors, where their dimensions gracefully change, revealing new facets of their nature.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for reshaping tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Reshape a tensor from size (2, 3) to (3, 2)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">reshaped_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Reshaped Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">reshaped_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 2],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [3, 4],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [5, 6]])</span>
</span></span></code></pre></div><h3 id="82-transposing-and-permuting-dimensions">8.2. Transposing and Permuting Dimensions</h3>
<p>Embark on a journey of transposition and permutation, where the dimensions of tensors rearrange in graceful dance.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for transposing and permuting dimensions</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Transpose a tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">transposed_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Transposed Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">transposed_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 4],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [2, 5],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [3, 6]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Permute dimensions of a tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">permuted_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Permuted Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">permuted_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 4],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [2, 5],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [3, 6]])</span>
</span></span></code></pre></div><h3 id="83-squeezing-and-unsqueezing">8.3. Squeezing and Unsqueezing</h3>
<p>Embrace the art of squeezing and unsqueezing, where the essence of tensors manifests in newfound ways.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for squeezing and unsqueezing tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Squeeze dimensions of size 1</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">]]])</span>
</span></span><span class="line"><span class="cl"><span class="n">squeezed_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Squeezed Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">squeezed_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 2, 3],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [4, 5, 6]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Unsqueeze and add dimensions of size 1</span>
</span></span><span class="line"><span class="cl"><span class="n">unsqueezed_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Unsqueezed Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">unsqueezed_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[[[1],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [2],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [3]]],</span>
</span></span><span class="line"><span class="cl"><span class="c1"># </span>
</span></span><span class="line"><span class="cl"><span class="c1"># </span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [[[4],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [5],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#           [6]]]])</span>
</span></span></code></pre></div><h3 id="84-flattening-and-raveling-tensors">8.4. Flattening and Raveling Tensors</h3>
<p>Venture into the realm of flattening and raveling, where tensors transform into elegant 1D forms.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for flattening and raveling tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Flatten a tensor into 1D</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">flattened_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Flattened Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">flattened_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 2, 3, 4, 5, 6])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Ravel a tensor into 1D</span>
</span></span><span class="line"><span class="cl"><span class="n">raveled_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Raveled Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">raveled_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 2, 3, 4, 5, 6])</span>
</span></span></code></pre></div><h3 id="85-concatenating-and-stacking-tensors">8.5. Concatenating and Stacking Tensors</h3>
<p>The art of concatenation and stacking unfolds before us, as tensors merge and form new structures.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for concatenating and stacking tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Concatenate tensors along a specific dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">concatenated_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Concatenated Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">concatenated_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 2, 3, 4, 5, 6])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Stack tensors along a new dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">stacked_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">,</span> <span class="n">tensor_c</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Stacked Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">stacked_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 2, 3],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [4, 5, 6],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [7, 8, 9]])</span>
</span></span></code></pre></div><p>May the intricacies of tensor reshaping and dimensionality reveal the beauty and depth of PyTorch tensors. As we journey forth, the mysteries of PyTorch continue to unravel, and the realm of deep learning awaits our exploration. Let us forge ahead with knowledge and wonder, unlocking the full potential of tensors in the captivating world of PyTorch.</p>
<h2 id="9-tensor-reduction-operations">9. Tensor Reduction Operations</h2>
<p>Our expedition now leads us to the realm of tensor reduction operations, where we explore methods to compute summation, mean, minimum, maximum, argmin, and argmax of tensors. Prepare to witness the reduction of tensor dimensions, as we uncover the essence of data aggregation.</p>
<h3 id="91-summation-and-mean">9.1. Summation and Mean</h3>
<p>Observe the elegant art of summation and mean computation, as tensors reveal their collective values along specific dimensions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for summation and mean of tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Summation of all elements in a tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">sum_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Sum of All Elements in Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sum_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(21)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Mean along a specific dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Mean Along Dimension 0:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">mean_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([2.5000, 3.5000, 4.5000])</span>
</span></span></code></pre></div><h3 id="92-minimum-and-maximum">9.2. Minimum and Maximum</h3>
<p>Behold the enchanting discovery of minimum and maximum values residing within tensors.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for finding minimum and maximum values in tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Find the minimum and maximum values in a tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">min_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">max_value</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Minimum Value in Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">min_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(1)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Maximum Value in Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">max_value</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(6)</span>
</span></span></code></pre></div><h3 id="93-argmin-and-argmax">9.3. Argmin and Argmax</h3>
<p>Unravel the mysteries of indices where minimum and maximum values reside within tensors.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for finding indices of minimum and maximum values in tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Find the indices of minimum and maximum values in a tensor</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">argmin_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">argmax_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Indices of Minimum Value:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">argmin_indices</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(0)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Indices of Maximum Value:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">argmax_indices</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(5)</span>
</span></span></code></pre></div><h3 id="94-reductions-along-specific-axes">9.4. Reductions Along Specific Axes</h3>
<p>Delve into the art of reductions, where tensors harmoniously condense along specific axes.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for reductions along specific axes of tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Perform reductions along specific axes</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Convert the tensor to floating-point data type</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">sum_along_rows</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">mean_along_columns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tensor_example</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Sum Along Rows:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">sum_along_rows</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([6., 15.])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Mean Along Columns:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">mean_along_columns</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([2.5000, 3.5000, 4.5000])</span>
</span></span></code></pre></div><h3 id="95-logical-reductions-all-any">9.5. Logical Reductions (All, Any)</h3>
<p>Venture into the realm of logical reductions, where we unveil the truth behind tensors.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for logical reductions in tensors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check if all elements are non-zero</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">all_non_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">tensor_example</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Are All Elements Non-Zero?&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">all_non_zero</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(True)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Check if any element is non-zero</span>
</span></span><span class="line"><span class="cl"><span class="n">any_non_zero</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tensor_example</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Is Any Element Non-Zero?&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">any_non_zero</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor(True)</span>
</span></span></code></pre></div><h2 id="10-gradient-computation-and-autograd">10. Gradient Computation and Autograd</h2>
<p>Our expedition now turns towards the mystical domain of gradient computation and autograd in PyTorch. As we delve deeper into the magical world of automatic differentiation, we unlock the power to compute gradients for fine-tuning deep learning models.</p>
<h3 id="101-automatic-differentiation-in-pytorch">10.1. Automatic Differentiation in PyTorch</h3>
<p>Behold the enchanting concept of automatic differentiation, where PyTorch unveils the magic of computing gradients.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for automatic differentiation in PyTorch</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Enable gradient computation</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Perform operations for gradient computation</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">tensor_example</span> <span class="o">*</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Result of Tensor Operations:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([2., 4., 6.], grad_fn=&lt;MulBackward0&gt;)</span>
</span></span></code></pre></div><h3 id="102-computing-gradients-with-autograd">10.2. Computing Gradients with Autograd</h3>
<p>The journey continues as we harness the power of PyTorch&rsquo;s autograd to compute gradients.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for computing gradients with autograd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Compute gradients with autograd</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">tensor_example</span> <span class="o">*</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Access gradients</span>
</span></span><span class="line"><span class="cl"><span class="n">gradients</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">grad</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Gradients of the Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([2., 2., 2.])</span>
</span></span></code></pre></div><h3 id="103-detaching-tensors-from-autograd">10.3. Detaching Tensors from Autograd</h3>
<p>In the pursuit of precise control, we learn how to detach tensors from the computational graph to avoid tracking gradients.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for detaching tensors from autograd</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Detach tensors from autograd</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_example</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">detached_tensor</span> <span class="o">=</span> <span class="n">tensor_example</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Detached Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">detached_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1., 2., 3.])</span>
</span></span></code></pre></div><h3 id="104-working-with-require_grad-and-volatile">10.4. Working with <code>require_grad</code> and <code>volatile</code></h3>
<p>As we gain more mastery over gradient computation, we explore the usage of <code>require_grad</code> and <code>volatile</code> attributes for fine-grained control.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for working with require_grad and torch.no_grad()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use torch.no_grad() for tensors that do not require gradients</span>
</span></span><span class="line"><span class="cl"><span class="n">volatile_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Use require_grad for fine-grained gradient computation control</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Operation involving tensors with gradients</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">tensor_a</span> <span class="o">*</span> <span class="n">tensor_b</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Volatile Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">volatile_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1., 2., 3.])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Gradients will be computed for tensor_a because it has requires_grad=True</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Result with require_grad set:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([ 4., 10., 18.], grad_fn=&lt;MulBackward0&gt;)</span>
</span></span></code></pre></div><p>As our grand adventure comes to a close, we stand in awe of the vast landscape of PyTorch, where tensors lay the foundation for cutting-edge machine learning and deep learning models. Armed with this knowledge, we can wield the power of tensors and gradients to unlock the full potential of artificial intelligence and data-driven discoveries. The quest for knowledge never ends, and with PyTorch as our guide, we embark on new journeys to unravel the mysteries of the data universe.</p>
<h2 id="11-tensor-operations-in-advanced-topics">11. Tensor Operations in Advanced Topics</h2>
<p>As we ascend to more advanced territories, we delve into the realm of advanced broadcasting techniques and uncover the mysteries of Einstein summation (Einsum) notation.</p>
<h3 id="111-advanced-broadcasting-and-einsum">11.1. Advanced Broadcasting and Einsum</h3>
<p>Prepare to be mesmerized by the magic of advanced broadcasting, where tensors with different shapes align in harmonious unity.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for advanced broadcasting and Einsum notation</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Advanced broadcasting with tensors of different shapes</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_broadcast</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,i-&gt;ij&#39;</span><span class="p">,</span> <span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Advanced Broadcasting Result:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_broadcast</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[10, 20],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [30, 40]])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Einsum notation for tensor operations</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result_einsum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ij,i-&gt;ij&#39;</span><span class="p">,</span> <span class="n">tensor_c</span><span class="p">,</span> <span class="n">tensor_d</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Einsum Notation Result:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result_einsum</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[10, 20],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [30, 40]])</span>
</span></span></code></pre></div><h3 id="112-tensor-concatenation-and-splitting">11.2. Tensor Concatenation and Splitting</h3>
<p>Embrace the art of tensor unity through concatenation and explore the beauty of tensor division through splitting.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for tensor concatenation and splitting</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Concatenate tensors along specific dimensions</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">concatenated_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">tensor_a</span><span class="p">,</span> <span class="n">tensor_b</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Concatenated Tensor:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">concatenated_tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 2, 3, 4, 5, 6])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Split tensors along a specific dimension</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">split_tensors</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">tensor_c</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Split Tensors:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">split_tensors</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[1, 2, 3],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [4, 5, 6]])</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([], size=(0, 3))</span>
</span></span></code></pre></div><h3 id="113-masked-operations-and-scatter-gather">11.3. Masked Operations and Scatter-Gather</h3>
<p>Unveil the power of masked tensor operations and the intricacies of scatter-gather operations.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for masked operations and scatter-gather</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Masked tensor operations</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">masked_result</span> <span class="o">=</span> <span class="n">tensor_a</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Masked Result:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">masked_result</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 3])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Scatter and gather operations</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span> <span class="p">[</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">]])</span>
</span></span><span class="line"><span class="cl"><span class="n">gathered_values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">tensor_b</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Gathered Values:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">gathered_values</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([[10, 40],</span>
</span></span><span class="line"><span class="cl"><span class="c1">#         [ 5,  6]])</span>
</span></span></code></pre></div><h3 id="114-advanced-element-wise-operations">11.4. Advanced Element-wise Operations</h3>
<p>Embark on a journey through advanced element-wise operations, where we wield the power of condition-based element-wise selection and transcend with advanced mathematical functions.</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># Code examples for advanced element-wise operations</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Element-wise selection using a condition</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">condition</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">selected_elements</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">condition</span><span class="p">,</span> <span class="n">tensor_a</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Selected Elements:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">selected_elements</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([1, 0, 3, 0, 0])</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># Advanced element-wise mathematical functions</span>
</span></span><span class="line"><span class="cl"><span class="n">tensor_b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">tensor_b</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;Result of Logarithm (base 10) Function:&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># Output:</span>
</span></span><span class="line"><span class="cl"><span class="c1"># tensor([0.0000, 0.3010, 0.4771])</span>
</span></span></code></pre></div><h2 id="conclusion">Conclusion</h2>
<p>This comprehensive tutorial has delved into the fascinating world of PyTorch tensors, providing a rigorous and professional understanding of their intricacies. Tensors, as multi-dimensional arrays, extend beyond traditional scalars, vectors, and matrices, empowering us with powerful tools for deep learning.</p>
<p>Throughout our journey, we explored various aspects of tensors, from their creation using Python lists and NumPy arrays to their serialization and I/O for model reproducibility. We learned to manipulate tensors through slicing, reshaping, and dimensionality transformations, gaining insight into their attributes and metadata.</p>
<p>The tutorial also covered essential tensor operations, including arithmetic computations, mathematical functions, and logical comparisons. We mastered tensor broadcasting, allowing element-wise operations on tensors of different shapes, and discovered the art of reduction operations for data aggregation.</p>
<p>Furthermore, we dived into gradient computation and automatic differentiation, crucial for fine-tuning deep learning models. We also explored advanced topics such as tensor broadcasting, masked operations, scatter-gather operations, and advanced element-wise computations, empowering us to work with tensors with precision and flexibility.</p>
<p>Armed with this knowledge, you are now equipped to navigate the vast landscape of PyTorch tensors and embark on exciting endeavors in the fields of machine learning and artificial intelligence. The power of tensors, seamlessly integrated into PyTorch, will fuel your future research and lead you to data-driven discoveries.</p>
<p>As you continue your academic journey, may PyTorch tensors serve as your trusted guide, enabling you to unravel the mysteries of the data universe and unlock the full potential of deep learning models. The pursuit of knowledge is endless, and with PyTorch tensors as your ally, you are poised for success in the captivating world of data science and machine learning. The realm of PyTorch awaits your exploration; the journey continues!</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/pytorch/">PyTorch</a>
  
</div>



<div class="share-box">
  <ul class="share">
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Farmanasq.github.io%2FDeep-Learning%2FPyTorch-Tensors%2F&amp;text=A&#43;Profound&#43;Journey&#43;into&#43;PyTorch&#43;Tensors%3A&#43;A&#43;Comprehensive&#43;Tutorial" target="_blank" rel="noopener" class="share-btn-twitter" aria-label="twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https%3A%2F%2Farmanasq.github.io%2FDeep-Learning%2FPyTorch-Tensors%2F&amp;t=A&#43;Profound&#43;Journey&#43;into&#43;PyTorch&#43;Tensors%3A&#43;A&#43;Comprehensive&#43;Tutorial" target="_blank" rel="noopener" class="share-btn-facebook" aria-label="facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
        
      
      <li>
        <a href="mailto:?subject=A%20Profound%20Journey%20into%20PyTorch%20Tensors%3A%20A%20Comprehensive%20Tutorial&amp;body=https%3A%2F%2Farmanasq.github.io%2FDeep-Learning%2FPyTorch-Tensors%2F" target="_blank" rel="noopener" class="share-btn-email" aria-label="envelope">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https%3A%2F%2Farmanasq.github.io%2FDeep-Learning%2FPyTorch-Tensors%2F&amp;title=A&#43;Profound&#43;Journey&#43;into&#43;PyTorch&#43;Tensors%3A&#43;A&#43;Comprehensive&#43;Tutorial" target="_blank" rel="noopener" class="share-btn-linkedin" aria-label="linkedin-in">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="whatsapp://send?text=A&#43;Profound&#43;Journey&#43;into&#43;PyTorch&#43;Tensors%3A&#43;A&#43;Comprehensive&#43;Tutorial%20https%3A%2F%2Farmanasq.github.io%2FDeep-Learning%2FPyTorch-Tensors%2F" target="_blank" rel="noopener" class="share-btn-whatsapp" aria-label="whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https%3A%2F%2Farmanasq.github.io%2FDeep-Learning%2FPyTorch-Tensors%2F&amp;title=A&#43;Profound&#43;Journey&#43;into&#43;PyTorch&#43;Tensors%3A&#43;A&#43;Comprehensive&#43;Tutorial" target="_blank" rel="noopener" class="share-btn-weibo" aria-label="weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://armanasq.github.io/"><img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu423262b037e945bf3d00a3d75617f940_247637_270x270_fill_q75_lanczos_center.jpeg" alt="Arman Asgharpoor Golroudbari"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://armanasq.github.io/">Arman Asgharpoor Golroudbari</a></h5>
      <h6 class="card-subtitle">Space-AI Researcher</h6>
      <p class="card-text">My research interests revolve around planetary rovers and spacecraft vision-based navigation.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-comment-alt"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:a.asgharpoor1993@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=IlAgF9UAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/armanasq" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/asgharpoor" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://orcid.org/my-orcid?orcid=0000-0001-6271-4533" target="_blank" rel="noopener">
        <i class="ai ai-orcid"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://www.webofscience.com/wos/author/record/IAN-3152-2023" target="_blank" rel="noopener">
        <i class="ai ai-publons"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://researchgate.net/profile/Arman_Asgharpoor" target="_blank" rel="noopener">
        <i class="ai ai-researchgate"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/uploads/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  

  

  
  <section id="comments">
    
  
  <script src="https://giscus.app/client.js"
          data-repo="Armanasq/Armanasq.github.io"
          data-repo-id="R_kgDOJi13ZQ"
          data-category="[ENTER CATEGORY NAME HERE]"
          data-category-id="[ENTER CATEGORY ID HERE]"
          data-mapping="pathname"
          data-strict="0"
          data-reactions-enabled="1"
          data-emit-metadata="0"
          data-input-position="top"
          data-theme="preferred_color_scheme"
          data-lang="en"
          data-loading="lazy"
          crossorigin="anonymous"
          async>
  </script>


  </section>
  










  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    Â© 2024 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> â€” the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.85070d5fe00d43eaedff44310b81dc2c.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js" type="module"></script>













  
    
      
      <!DOCTYPE html>
<html>
<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-55GQYC5GYC"></script>

<style>
  .myImg {
    border-radius: 5px;
    cursor: pointer;
    transition: 0.3s;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  .myImg:hover {
    opacity: 0.7;
    cursor: pointer;
  }

   
  .modal-img {
    display: none;  
    position: fixed;  
    z-index: 1;  
    padding-top: 150px;  
    left: 0;
    top: 0px;
    width: 100%;  
    height: 100%;  
    overflow: visible;  
    background-color: rgb(0, 0, 0);  
    background-color: rgba(0, 0, 0, 0.6);  
    margin-left: auto;
    margin-right: auto;
  }

   
  .modal-content {
    display: block;
    margin-left: auto;
    margin-right: auto;
    max-width: 80%;
    max-height: 80%;

  }

   
  #caption {
    margin-left: auto;
    margin-right: auto;
    width: 80%;
    max-width: 700px;
    text-align: center;
    padding: 10px 0;

  }

   
  .modal-content,
  #caption {
    -webkit-animation-name: zoom;
    -webkit-animation-duration: 0.6s;
    animation-name: zoom;
    animation-duration: 0.6s;
    margin-left: auto;
    margin-right: auto;
  }

  @-webkit-keyframes zoom {
    from {
      -webkit-transform: scale(0);
    }
    to {
      -webkit-transform: scale(1);
    }
  }

  @keyframes zoom {
    from {
      transform: scale(0);
    }
    to {
      transform: scale(1);
    }
  }

   
  
 .modal-close {
    position: absolute;
    top: -55px;
    right: 0;
    font-size: 40px;
    font-weight: bold;
    transition: 0.3s;
    cursor: pointer;
  }

  .modal-close:hover,
  .modal-close:focus {
    color: #bbb;
    text-decoration: none;
  }

   
  @media only screen and (max-width: 900px) {
    .modal-content {
      width: 90%;
    }
  }

  .test:hover {
    scale: 1.2;
  }







  .navbar-nav {
    font-size:20px;
    font-family: Merriweather,sans-serif;
  }

  .robotic-section-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: space-between;
    max-width: 1200px;
    margin: 0 auto;
  }
  
  .robotic-section {
    flex-basis: calc(30.33% - 12px);
    margin: 10px;
    background-color: #fff;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    transition: box-shadow 0.3s ease-in-out;
    overflow: hidden;
  }
  
  .robotic-section:hover {
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
  }
  
  .robotic-section-content {
    text-align: center;
    padding: 20px;
    display: flex;
    flex-direction: column;
    justify-content: space-between;
    height: 100%;
  }
  
  .robotic-section-content .image-placeholder {
    width: 300px;
    height: 300px;
    margin: 0 auto;
    display: flex;
    align-items: center;
    justify-content: center;
    background-color: #f1f1f1;
  }
  
  .robotic-section-content .image-placeholder img {
    max-width: 100%;
    max-height: 100%;
    object-fit: contain;
  }
  
  .robotic-section-content-h2 {
    margin-top: 10px;
    font-size: 1.rem;
    font-weight: bold;
    color: #333;
  }
  
  .robotic-section-content-h2 :hover{
    font-size: 10px
  }
  .robotic-section-content-h2 {
    margin-top: 10px;
    color: #777;
    font-size: 1.2rem;
  }
  
  .robotic-section-content .text-placeholder {
    height: 80px;
    background-color: #f1f1f1;
  }
  
  .robotic-section-content a {
    display: inline-block;
    margin-top: 20px;
    padding: 10px 20px;
    background-color: #FF4081;
    color: #fff;
    text-decoration: none;
    border-radius: 4px;
    font-weight: bold;
    transition: background-color 0.3s ease-in-out;
  }
  
  .robotic-section-content a:hover {
    background-color: #E91E63;
  }
  
   
  @media (max-width: 768px) {
    .robotic-section {
      flex-basis: calc(50% - 40px);
    }
  }
  
   
  @media (max-width: 480px) {
    .robotic-section {
      flex-basis: 100%;
    }
  }
</style>
</head>
<body>

<div id="myModal" class="modal-img">
  <div class="modal-content">
    <span class="modal-close">&times;</span>
    <img id="img01" style="margin-left: auto; margin-right: auto;">
    <div id="caption"></div>
  </div>
</div>




<script>
    
    var modal = document.getElementById("myModal");
    
    
    var images = document.querySelectorAll("img.myImg");
    
    
    var modalImg = document.getElementById("img01");
    var captionText = document.getElementById("caption");
    
    
    for (var i = 0; i < images.length; i++) {
      
      images[i].setAttribute("data-src", images[i].src);
      
      images[i].addEventListener("click", function() {
        
        modalImg.src = this.getAttribute("data-src");
        captionText.innerHTML = this.alt;
        
        modal.style.display = "block";
      });
    }
    
    
    var modalClose = document.querySelector(".modal-content .modal-close");
    
    
    modalClose.onclick = function() {
      modal.style.display = "none";
    };
    
















    
    </script>
    
    </body>
    </html>
      
    
  






</body>
</html>
